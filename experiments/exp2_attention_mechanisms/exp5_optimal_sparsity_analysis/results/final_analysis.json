{
  "experiment_summary": {
    "total_experiments": 24,
    "sequence_lengths_tested": [
      64,
      128,
      256,
      512
    ],
    "sparsity_ratios_tested": [
      0.25,
      0.33,
      0.5,
      0.67,
      0.75,
      0.9
    ],
    "runs_per_config": 3,
    "model_config": {
      "d_model": 256,
      "n_heads": 8,
      "n_layers": 4,
      "d_ff": 1024
    }
  },
  "optimal_sparsity_results": {
    "64": {
      "optimal_sparsity_ratio": 0.25,
      "optimal_top_k": 48,
      "optimal_val_loss": 18.0897815294233,
      "optimal_val_accuracy": 0.1987021123119517,
      "worst_sparsity_ratio": 0.75,
      "worst_val_loss": 18.97240813676589,
      "improvement_percentage": 4.652159077435104
    },
    "128": {
      "optimal_sparsity_ratio": 0.25,
      "optimal_top_k": 96,
      "optimal_val_loss": 16.775929076757304,
      "optimal_val_accuracy": 0.2038488576816528,
      "worst_sparsity_ratio": 0.75,
      "worst_val_loss": 17.828917617087395,
      "improvement_percentage": 5.90607104113206
    },
    "256": {
      "optimal_sparsity_ratio": 0.25,
      "optimal_top_k": 192,
      "optimal_val_loss": 15.742391276890226,
      "optimal_val_accuracy": 0.21458030462535926,
      "worst_sparsity_ratio": 0.75,
      "worst_val_loss": 16.680633972261802,
      "improvement_percentage": 5.624742422450959
    },
    "512": {
      "optimal_sparsity_ratio": 0.25,
      "optimal_top_k": 384,
      "optimal_val_loss": 14.825591954097177,
      "optimal_val_accuracy": 0.22064684274896126,
      "worst_sparsity_ratio": 0.75,
      "worst_val_loss": 15.906269813315701,
      "improvement_percentage": 6.794037017490109
    }
  },
  "statistical_analysis": {
    "64": {
      "loss_range": 0.8826266073425906,
      "loss_std": 0.33357487104592054,
      "accuracy_range": 0.11994958971279897,
      "accuracy_std": 0.04677106081380024
    },
    "128": {
      "loss_range": 1.052988540330091,
      "loss_std": 0.387368065121271,
      "accuracy_range": 0.125602867060512,
      "accuracy_std": 0.04954144745059736
    },
    "256": {
      "loss_range": 0.9382426953715761,
      "loss_std": 0.36534588344609514,
      "accuracy_range": 0.13667919402947665,
      "accuracy_std": 0.0528826639279603
    },
    "512": {
      "loss_range": 1.0806778592185236,
      "loss_std": 0.4196316406340361,
      "accuracy_range": 0.14448000697376356,
      "accuracy_std": 0.05475783478729749
    }
  },
  "key_insights": [
    "Optimal sparsity ratio is consistently 0.25 (75% of tokens) across all tested sequence lengths",
    "Performance degrades significantly with higher sparsity ratios (>0.5)",
    "The 0.25 sparsity ratio provides the best balance between efficiency and performance",
    "Longer sequences show more dramatic performance differences between sparsity levels",
    "Training time remains relatively constant across sparsity ratios, indicating sparse attention overhead is minimal"
  ]
}