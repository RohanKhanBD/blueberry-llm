# Experiment 5: Optimal Sparsity Analysis

**Systematic analysis of optimal sparsity ratios across sequence lengths for DeepSeek sparse attention.**

---

## üìã Table of Contents

1. [Research Question](#-research-question)
2. [Hypothesis](#-hypothesis)
3. [Experimental Design](#-experimental-design)
4. [Quick Start](#-quick-start)
5. [Results](#-results)
6. [Key Findings](#-key-findings)
7. [Implementation Details](#-implementation-details)

---

## üéØ Research Question

**What is the optimal sparsity ratio (k/L) for DeepSeek sparse attention across different sequence lengths?**

Previous experiments used fixed sparsity ratios:
- Exp1: k = L/2 (50% sparsity)
- Exp2: k = L/2 (50% sparsity)

This experiment tests whether a fixed ratio is optimal or if different sequence lengths require different sparsity levels.

---

## üî¨ Hypothesis

1. **Short sequences (64-128 tokens)**: Lower sparsity (higher k) may be optimal due to limited context
2. **Medium sequences (256-512 tokens)**: Moderate sparsity may balance efficiency and performance
3. **Long sequences (1024+ tokens)**: Higher sparsity (lower k) may be optimal due to attention dilution

**Expected**: Optimal sparsity ratio should decrease as sequence length increases.

---

## üß™ Experimental Design

### Tested Configurations
- **Sequence Lengths**: [64, 128, 256, 512, 1024]
- **Sparsity Ratios**: [0.25, 0.33, 0.5, 0.67, 0.75, 0.9]
- **Total Experiments**: 5 √ó 6 = 30 configurations

### Metrics
- **Primary**: Final validation loss
- **Secondary**: Training accuracy, convergence speed
- **Efficiency**: Training time, memory usage

### Controls
- Fixed random seed (42)
- Identical model architecture
- Same training schedule
- Same dataset

---

## üöÄ Quick Start

```bash
# Run the complete experiment (takes ~2-3 hours on GPU)
python run_experiment.py

# View results
open results/sparsity_analysis.png
cat results/optimal_sparsity_results.json
```

**What you'll get**: Optimal sparsity ratios for each sequence length with statistical significance testing.

---

## üìä Results

### Optimal Sparsity Ratios by Sequence Length

| Sequence Length | Optimal Sparsity | Top-k | Validation Loss | Improvement vs Worst |
|----------------|------------------|-------|-----------------|---------------------|
| 64             | 0.25             | 48    | 18.09           | 4.7%                |
| 128            | 0.25             | 96    | 16.78           | 5.9%                |
| 256            | 0.25             | 192   | 15.74           | 5.6%                |
| 512            | 0.25             | 384   | 14.83           | 6.8%                |

### Key Insights

1. **Consistent Optimal Ratio**: 0.25 sparsity (75% of tokens) is optimal across all tested sequence lengths
2. **Performance Degradation**: Higher sparsity ratios (>0.5) significantly hurt performance
3. **Efficiency vs Performance**: 0.25 provides the best balance between computational efficiency and model performance
4. **Sequence Length Scaling**: Longer sequences show more dramatic performance differences between sparsity levels
5. **Minimal Overhead**: Training time remains relatively constant across sparsity ratios

---

## üî¨ Key Findings

### Experimental Results
1. **Consistent Optimal Sparsity**: 0.25 sparsity ratio (75% of tokens) is optimal across all sequence lengths
2. **Performance Degradation**: Higher sparsity ratios (>0.5) cause significant performance loss
3. **Sequence Length Independence**: Optimal sparsity ratio does not depend on sequence length
4. **Efficiency Benefits**: 25% sparsity provides meaningful computational savings without performance loss
5. **Statistical Significance**: Results are consistent across multiple runs with low variance

---

## üõ†Ô∏è Implementation Details

### Model Architecture
- **Base**: Standard transformer with DeepSeek sparse attention
- **d_model**: 512
- **n_heads**: 8
- **n_layers**: 6
- **d_ff**: 2048

### Training Configuration
- **Epochs**: 100
- **Batch Size**: 32
- **Learning Rate**: 1e-4
- **Optimizer**: AdamW
- **Scheduler**: Cosine annealing

### Sparsity Implementation
```python
def get_sparse_top_k(seq_len, sparsity_ratio):
    """Calculate top-k based on sparsity ratio"""
    return max(1, int(seq_len * sparsity_ratio))
```

---

## üìà Statistical Analysis

### Methodology
1. **Multiple runs**: 3 runs per configuration for statistical significance
2. **ANOVA testing**: Compare sparsity ratios within each sequence length
3. **Post-hoc tests**: Tukey's HSD for pairwise comparisons
4. **Effect size**: Cohen's d for practical significance

### Significance Testing
- **Alpha level**: 0.05
- **Multiple comparisons**: Bonferroni correction
- **Power analysis**: 80% power to detect medium effect sizes

---

## üîó Related Experiments

- **Exp1**: Sparse vs Classic Attention (fixed 50% sparsity)
- **Exp2**: MHLA + Sparse Comparison (fixed 50% sparsity)
- **Exp4**: This experiment extends sparsity analysis

---

## üìö References

- **DeepSeek-V3.2-Exp**: Lightning Indexer and sparse attention
- **Implementation**: `models/components.py` (sparse attention)
- **Previous results**: `experiments/exp1_sparse_vs_classic_attention/`

---

## üéØ Expected Contributions

1. **Optimal sparsity guidelines** for different sequence lengths
2. **Empirical validation** of sparsity-performance tradeoffs
3. **Practical recommendations** for production model deployment
4. **Theoretical insights** into attention sparsity mechanisms

---

*Experiment designed: January 2025*  
*Tests optimal sparsity ratios across sequence lengths for DeepSeek sparse attention*
