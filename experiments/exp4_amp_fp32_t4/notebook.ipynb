{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwBa9uKsrAB2"
      },
      "source": [
        "# T4 AMP vs FP32 Optimized Research Pipeline\n",
        "\n",
        "## Research Question\n",
        "Under what conditions does Automatic Mixed Precision (AMP) actually improve performance on a T4 GPU?\n",
        "\n",
        "## Performance Optimizations\n",
        "- Intelligent grid search with adaptive filtering\n",
        "- Experiment caching to eliminate duplicate work\n",
        "- Model pooling to reduce initialization overhead\n",
        "- Parallel execution with efficient resource management\n",
        "- Statistical early stopping to reduce unnecessary repeats\n",
        "- Phased approach (coarse â†’ fine search)\n",
        "\n",
        "## Expected Performance Improvement: 6-10x faster execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMf3k2ZQ8sY8"
      },
      "outputs": [],
      "source": [
        "# Installation and Setup\n",
        "!pip install -q transformers accelerate datasets pandas seaborn nvidia-ml-py matplotlib tqdm\n",
        "\n",
        "import os\n",
        "# Make the allocator resilient and disable cudagraphs globally to avoid overwrite errors.\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True,max_split_size_mb:128\")\n",
        "os.environ.setdefault(\"CUDA_LAUNCH_BLOCKING\", \"0\")\n",
        "# Some builds honor this env var for a full cudagraphs kill-switch:\n",
        "os.environ.setdefault(\"TORCHINDUCTOR_DISABLE_CUDAGRAPHS\", \"1\")\n",
        "\n",
        "import torch\n",
        "import time\n",
        "import json\n",
        "import csv\n",
        "import itertools\n",
        "import math\n",
        "import random\n",
        "import shutil\n",
        "import zipfile\n",
        "import threading\n",
        "import pickle\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from pathlib import Path\n",
        "\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pynvml\n",
        "import torch.profiler as prof\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "from transformers.models.glm4.configuration_glm4 import Glm4Config\n",
        "from datasets import load_dataset\n",
        "\n",
        "# --- IMPORTANT: Disable Inductor CUDA Graphs to avoid tensor overwrite errors ---\n",
        "try:\n",
        "    from torch._inductor import config as inductor_config\n",
        "    # Older flags:\n",
        "    inductor_config.triton.cudagraphs = False\n",
        "    # Newer flag (if present):\n",
        "    if hasattr(inductor_config, \"cuda\"):\n",
        "        inductor_config.cuda.disable_cudagraphs = True\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# (Optional) Quiet a couple of noisy but benign warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"Online softmax is disabled\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"_maybe_guard_rel\")\n",
        "\n",
        "# Configuration\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "device = 'cuda'\n",
        "pynvml.nvmlInit()\n",
        "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "CACHE_DIR = \"experiment_cache\"\n",
        "ARTEFACTS_DIR = \"artefacts\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd-D4QLqrAB4"
      },
      "source": [
        "## Hyperparameter Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAg2bSr5rAB4",
        "outputId": "5796e66f-c500-47bb-9e54-d92563814e51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”§ Configuration complete!\n",
            "ðŸ“Š Total experiments in *initial coarse grid*: 48\n",
            "ðŸ“Š Total experiments in *full potential grid*: 720\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration (RAM-safe toggles) ---\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "device = 'cuda'\n",
        "pynvml.nvmlInit()\n",
        "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "CACHE_DIR = \"experiment_cache\"\n",
        "ARTEFACTS_DIR = \"artefacts\"\n",
        "\n",
        "# --- Model Configuration ---\n",
        "LAYERS_LIST     = [4, 8, 12]\n",
        "D_MODEL_LIST    = [128, 256, 512]\n",
        "EXPERTS         = 4\n",
        "TOPK            = 2\n",
        "VOCAB           = 32000\n",
        "\n",
        "# --- Data Configuration ---\n",
        "BS_LIST         = [1, 2, 4, 8, 16]\n",
        "SEQLEN_LIST     = [64, 128, 256, 512]\n",
        "\n",
        "# --- Training Configuration ---\n",
        "PREC_LIST       = ['fp32', 'amp']\n",
        "REPEATS         = 5\n",
        "WARMUP          = 3\n",
        "\n",
        "# --- System Configuration ---\n",
        "COMPILE_LIST    = [False]\n",
        "STATIC_LIST     = [False, True]\n",
        "\n",
        "# --- Optimizer Configuration ---\n",
        "FUSED_ADAM      = True\n",
        "FOREACH         = False\n",
        "LR              = 1e-4\n",
        "OPTIMIZER_PRIMARY   = \"adamw\"\n",
        "OPTIMIZER_FALLBACK  = \"sgd\"\n",
        "\n",
        "# --- Profiler & Energy Configuration ---\n",
        "ENABLE_PROF     = False\n",
        "POWER_SAMPLE_MS = 50    # interval for polling (still used, but we don't store an array)\n",
        "\n",
        "# --- Optimization Parameters ---\n",
        "MAX_WORKERS     = 1\n",
        "MIN_REPEATS     = 3\n",
        "CONFIDENCE_THRESHOLD = 0.1\n",
        "PROFILE_RATE    = 0.05\n",
        "MAX_PROFILES    = 10\n",
        "\n",
        "# --- Coarse-to-Fine Search (keep coarse small) ---\n",
        "INITIAL_LAYERS_LIST     = [4, 12]\n",
        "INITIAL_D_MODEL_LIST    = [128]\n",
        "INITIAL_BS_LIST         = [2, 8, 16]\n",
        "INITIAL_SEQLEN_LIST     = [64, 256]\n",
        "INITIAL_COMPILE_LIST    = [False]\n",
        "INITIAL_STATIC_LIST     = [False, True]\n",
        "\n",
        "# --- NEW: RAM saver switches ---\n",
        "STORE_TIMES_PER_RUN     = False   # don't keep run timing arrays in result dicts\n",
        "STORE_ENV_PER_EXPERIMENT= False   # include env info only in PR summary, not every result\n",
        "\n",
        "print(\"ðŸ”§ Configuration complete!\")\n",
        "initial_coarse_total = len(INITIAL_LAYERS_LIST) * len(INITIAL_D_MODEL_LIST) * len(INITIAL_BS_LIST) * len(INITIAL_SEQLEN_LIST) * len(PREC_LIST) * len(INITIAL_COMPILE_LIST) * len(INITIAL_STATIC_LIST)\n",
        "print(f\"ðŸ“Š Total experiments in *initial coarse grid*: {initial_coarse_total}\")\n",
        "full_grid_total = len(LAYERS_LIST) * len(D_MODEL_LIST) * len(BS_LIST) * len(SEQLEN_LIST) * len(PREC_LIST) * len(COMPILE_LIST) * len(STATIC_LIST)\n",
        "print(f\"ðŸ“Š Total experiments in *full potential grid*: {full_grid_total}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHVEsFqDrAB4"
      },
      "source": [
        "## Core Classes and Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLvUHxqhrAB5"
      },
      "outputs": [],
      "source": [
        "# --- Core Classes & Utilities (robust pool + improved mem estimators) ---\n",
        "from collections import deque\n",
        "\n",
        "class ExperimentCache:\n",
        "    def __init__(self, cache_dir=CACHE_DIR, window=64):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.ndjson_path = self.cache_dir / \"results.ndjson\"\n",
        "        self.keys_path   = self.cache_dir / \"completed_keys.txt\"\n",
        "        self.recent = deque(maxlen=window)\n",
        "        self._completed = set()\n",
        "        self._load_completed_keys()\n",
        "\n",
        "    def _load_completed_keys(self):\n",
        "        if self.keys_path.exists():\n",
        "            with open(self.keys_path, \"r\") as f:\n",
        "                for line in f:\n",
        "                    k = line.strip()\n",
        "                    if k:\n",
        "                        self._completed.add(k)\n",
        "\n",
        "    def _append_completed_key(self, k: str):\n",
        "        if k in self._completed: return\n",
        "        self._completed.add(k)\n",
        "        with open(self.keys_path, \"a\") as f:\n",
        "            f.write(k + \"\\n\")\n",
        "\n",
        "    def add_result(self, result: dict):\n",
        "        small = dict(result)\n",
        "        if not STORE_TIMES_PER_RUN:\n",
        "            small.pop('times', None)\n",
        "        if not STORE_ENV_PER_EXPERIMENT:\n",
        "            small.pop('env', None)\n",
        "        with open(self.ndjson_path, \"a\") as f:\n",
        "            f.write(json.dumps(small) + \"\\n\")\n",
        "        self.recent.append(small)\n",
        "        if 'key' in small:\n",
        "            self._append_completed_key(small['key'])\n",
        "\n",
        "    def is_completed(self, experiment_key: str) -> bool:\n",
        "        return experiment_key in self._completed\n",
        "\n",
        "    def get_result(self, experiment_key: str):\n",
        "        for r in reversed(self.recent):\n",
        "            if r.get('key') == experiment_key:\n",
        "                return r\n",
        "        if not self.ndjson_path.exists():\n",
        "            return None\n",
        "        with open(self.ndjson_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    obj = json.loads(line)\n",
        "                    if obj.get('key') == experiment_key:\n",
        "                        return obj\n",
        "                except Exception:\n",
        "                    pass\n",
        "        return None\n",
        "\n",
        "    def read_all(self) -> list:\n",
        "        out = []\n",
        "        if not self.ndjson_path.exists():\n",
        "            return out\n",
        "        with open(self.ndjson_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    out.append(json.loads(line))\n",
        "                except Exception:\n",
        "                    pass\n",
        "        return out\n",
        "\n",
        "class ModelPool:\n",
        "    \"\"\"\n",
        "    T4-safe model pool:\n",
        "    - At most one resident model; evict on (layers,d_model,prec) change.\n",
        "    - Robust to partial failures (avoids KeyError on dict lookups).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.configs = {}\n",
        "        self.current_key = None\n",
        "\n",
        "    def _evict_all(self):\n",
        "        for k in list(self.models.keys()):\n",
        "            try:\n",
        "                del self.models[k]\n",
        "            except Exception:\n",
        "                pass\n",
        "        self.models.clear()\n",
        "        self.configs.clear()\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    def get_model(self, layers, d_model, prec):\n",
        "        key = f\"l{layers}_d{d_model}_p{prec}\"\n",
        "        if self.current_key != key:\n",
        "            # full eviction before (re)build to reduce fragmentation\n",
        "            self._evict_all()\n",
        "            try:\n",
        "                model, cfg = build_glm4_moe_model(\n",
        "                    layers, d_model, vocab_size=VOCAB, num_experts=EXPERTS, topk=TOPK\n",
        "                )\n",
        "                if (layers * d_model) >= 4096 and hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "                    try: model.gradient_checkpointing_enable()\n",
        "                    except Exception: pass\n",
        "                self.models[key] = model\n",
        "                self.configs[key] = cfg\n",
        "                self.current_key = key\n",
        "            except Exception as e:\n",
        "                # ensure clean state on failure, and surface a clear error\n",
        "                self._evict_all()\n",
        "                self.current_key = None\n",
        "                raise RuntimeError(f\"ModelPool build failed for {key}: {e}\")\n",
        "        # safe access (avoid KeyError)\n",
        "        model = self.models.get(key)\n",
        "        cfg   = self.configs.get(key)\n",
        "        if model is None or cfg is None:\n",
        "            # try once more cleanly\n",
        "            self._evict_all()\n",
        "            model, cfg = build_glm4_moe_model(\n",
        "                layers, d_model, vocab_size=VOCAB, num_experts=EXPERTS, topk=TOPK\n",
        "            )\n",
        "            self.models[key] = model\n",
        "            self.configs[key] = cfg\n",
        "            self.current_key = key\n",
        "        return model, cfg\n",
        "\n",
        "    def clear(self):\n",
        "        self._evict_all()\n",
        "        self.current_key = None\n",
        "\n",
        "class SmartProfiler:\n",
        "    def __init__(self, profile_rate=PROFILE_RATE, max_profiles=MAX_PROFILES):\n",
        "        self.profile_rate = profile_rate; self.max_profiles = max_profiles; self.profiles_done = 0\n",
        "    def should_profile(self, layers, d_model, bs, seqlen):\n",
        "        if self.profiles_done >= self.max_profiles: return False\n",
        "        complexity_score = (layers * d_model * bs * seqlen) / 1_000_000\n",
        "        return (0.5 <= complexity_score <= 5.0) and (random.random() < self.profile_rate)\n",
        "    def increment_profile_count(self): self.profiles_done += 1\n",
        "\n",
        "def statistical_early_stopping(times, min_repeats=MIN_REPEATS, confidence_threshold=CONFIDENCE_THRESHOLD):\n",
        "    if len(times) < min_repeats: return False, None\n",
        "    mean_time = np.mean(times); std_time = np.std(times)\n",
        "    cv = std_time / mean_time if mean_time > 0 else float('inf')\n",
        "    return (cv < confidence_threshold, (mean_time if cv < confidence_threshold else None))\n",
        "\n",
        "def optimize_gpu_memory():\n",
        "    torch.cuda.empty_cache(); torch.cuda.ipc_collect(); torch.cuda.reset_peak_memory_stats()\n",
        "    import gc; gc.collect()\n",
        "\n",
        "# ===== Improved memory estimators (account for attention O(seqlen^2)) =====\n",
        "def _num_heads_for(d_model: int) -> int:\n",
        "    return max(8, d_model // 64)\n",
        "\n",
        "def _estimate_params_moe(layers, d_model, num_experts=EXPERTS, vocab=VOCAB):\n",
        "    # attention ~ O(d^2), FFN MoE ~ O(E * d^2), very rough\n",
        "    per_layer = (4 + 8*num_experts) * (d_model**2)\n",
        "    total = layers * per_layer + vocab * d_model\n",
        "    return total\n",
        "\n",
        "def _estimate_param_mem_gb(params, dtype_bytes=4, with_adam=True):\n",
        "    mult = 3.0 if with_adam else 1.5  # params + grads (+ moments if Adam)\n",
        "    bytes_total = params * dtype_bytes * mult\n",
        "    return bytes_total / (1024**3)\n",
        "\n",
        "def _estimate_activation_mem_gb(layers, d_model, bs, seqlen, prec, topk=TOPK):\n",
        "    \"\"\"\n",
        "    New: include a quadratic attention term.\n",
        "    - MLP/MoE activations ~ O(layers * bs * seqlen * d * topk)\n",
        "    - Attention activations ~ O(layers * bs * seqlen^2 * d)   (queries/keys/values & attn maps)\n",
        "    \"\"\"\n",
        "    bytes_per = 2 if prec == 'amp' else 4\n",
        "    # MLP/MoE (linear-in-seqlen)\n",
        "    mlp_tokens = layers * bs * seqlen * d_model * max(1, topk)\n",
        "    # Attention (quadratic-in-seqlen). Use heads*d_head == d_model.\n",
        "    attn_tokens = layers * bs * (seqlen ** 2) * d_model\n",
        "    total_tokens = mlp_tokens + 2.0 * attn_tokens   # weight attn a bit more to be conservative\n",
        "    return (total_tokens * bytes_per) / (1024**3)\n",
        "\n",
        "def _needs_low_mem_optimizer(layers, d_model):\n",
        "    return (d_model >= 512) or ((layers * d_model) >= 4096)\n",
        "\n",
        "# ---- keep the make_opt shim (signature-compatible) ----\n",
        "def make_opt(*args, **kwargs):\n",
        "    if len(args) == 1 and isinstance(args[0], nn.Module):\n",
        "        model = args[0]\n",
        "        return torch.optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-8,\n",
        "                                 weight_decay=0.0, fused=FUSED_ADAM, foreach=FOREACH)\n",
        "    if len(args) >= 3 and isinstance(args[0], nn.Module):\n",
        "        model, layers, d_model = args[:3]\n",
        "        if OPTIMIZER_PRIMARY == \"adamw\" and not _needs_low_mem_optimizer(layers, d_model):\n",
        "            return torch.optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-8,\n",
        "                                     weight_decay=0.0, fused=FUSED_ADAM, foreach=FOREACH)\n",
        "        return torch.optim.SGD(model.parameters(), lr=LR)\n",
        "    raise TypeError(\"make_opt expects (model) or (model, layers, d_model)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVcvFNMfrAB5"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW-KwzFrrAB5"
      },
      "outputs": [],
      "source": [
        "\"\"\"## Helper Functions\"\"\"\n",
        "\n",
        "# NOTE: We define seed_all() here so it's always in scope for run_one_optimized().\n",
        "# This avoids \"name 'seed_all' is not defined\" if earlier setup cells weren't rerun.\n",
        "\n",
        "STRICT_TOKEN_CHECKS = True\n",
        "\n",
        "def seed_all():\n",
        "    \"\"\"Seed all RNGs for reproducibility.\"\"\"\n",
        "    try:\n",
        "        import random\n",
        "        import numpy as _np\n",
        "        import torch as _torch\n",
        "        _SEED = globals().get(\"RANDOM_SEED\", 42)\n",
        "        random.seed(_SEED)\n",
        "        _np.random.seed(_SEED)\n",
        "        _torch.manual_seed(_SEED)\n",
        "        if _torch.cuda.is_available():\n",
        "            _torch.cuda.manual_seed_all(_SEED)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def get_real_batch(bs: int, seqlen: int, vocab_size: int = None):\n",
        "    if vocab_size is None:\n",
        "        vocab_size = VOCAB\n",
        "    input_ids = torch.randint(\n",
        "        low=0, high=vocab_size, size=(bs, seqlen),\n",
        "        device=device, dtype=torch.long\n",
        "    )\n",
        "    return input_ids\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_batch(x: torch.Tensor, vocab_size: int):\n",
        "    if not STRICT_TOKEN_CHECKS:\n",
        "        return\n",
        "    if x.dtype != torch.long:\n",
        "        raise TypeError(f\"input_ids dtype must be torch.long, got {x.dtype}\")\n",
        "    if not x.is_cuda:\n",
        "        raise ValueError(\"input_ids must be on CUDA\")\n",
        "    x_min = int(x.min().item())\n",
        "    x_max = int(x.max().item())\n",
        "    if x_min < 0 or x_max >= vocab_size:\n",
        "        raise ValueError(\n",
        "            f\"Token id out of range [0, {vocab_size-1}]: min={x_min} max={x_max}\"\n",
        "        )\n",
        "\n",
        "def make_opt(model: nn.Module):\n",
        "    return torch.optim.AdamW(\n",
        "        model.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-8,\n",
        "        weight_decay=0.0, fused=FUSED_ADAM, foreach=FOREACH\n",
        "    )\n",
        "\n",
        "def safe_clip_grad_norm_(parameters, max_norm):\n",
        "    \"\"\"Clip only real, floating-point grads to avoid dtype/nullptr issues.\"\"\"\n",
        "    params = [p for p in parameters\n",
        "              if p.grad is not None and torch.is_floating_point(p.grad)]\n",
        "    if not params:\n",
        "        return 0.0\n",
        "    return torch.nn.utils.clip_grad_norm_(params, max_norm)\n",
        "\n",
        "def train_step(model: nn.Module, opt: torch.optim.Optimizer,\n",
        "              scaler: torch.amp.GradScaler | None, input_ids: torch.Tensor,\n",
        "              prec: str, prof_handler=None) -> float:\n",
        "    vocab_size = model.config.vocab_size if hasattr(model, 'config') else VOCAB\n",
        "    validate_batch(input_ids, vocab_size)\n",
        "    input_ids = input_ids.contiguous()\n",
        "\n",
        "    # IMPORTANT: use set_to_none=False so grads are real (float) tensors, not None.\n",
        "    # This avoids \"dtype nullptr vs float\" in fused/foreach optimizer paths or clipping.\n",
        "    opt.zero_grad(set_to_none=False)\n",
        "\n",
        "    use_amp = (prec == 'amp')\n",
        "    with torch.amp.autocast('cuda', enabled=use_amp, dtype=torch.float16):\n",
        "        out = model(input_ids=input_ids, labels=input_ids)\n",
        "        loss = out.loss\n",
        "\n",
        "    if use_amp:\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(opt)\n",
        "        safe_clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "    else:\n",
        "        loss.backward()\n",
        "        safe_clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "    if prof_handler is not None:\n",
        "        try:\n",
        "            prof_handler.step()\n",
        "        except Exception:\n",
        "            pass\n",
        "    return float(loss.detach().item())\n",
        "\n",
        "def train_step_with_grad_accum(model: nn.Module, opt: torch.optim.Optimizer,\n",
        "                             scaler: torch.amp.GradScaler | None, input_ids: torch.Tensor,\n",
        "                             prec: str, grad_accum_steps: int = 1, prof_handler=None) -> float:\n",
        "    \"\"\"\n",
        "    Performs a training step with optional gradient accumulation.\n",
        "\n",
        "    We also mark a cudagraph step boundary (no-op if cudagraphs are disabled)\n",
        "    to be robust with torch.compile().\n",
        "    \"\"\"\n",
        "    try:\n",
        "        torch.compiler.cudagraph_mark_step_begin()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if grad_accum_steps == 1:\n",
        "        return train_step(model, opt, scaler, input_ids, prec, prof_handler)\n",
        "\n",
        "    micro_batch_size = input_ids.size(0) // grad_accum_steps\n",
        "    total_loss = 0.0\n",
        "    for i in range(grad_accum_steps):\n",
        "        start_idx = i * micro_batch_size\n",
        "        end_idx = start_idx + micro_batch_size\n",
        "        micro_input_ids = input_ids[start_idx:end_idx]\n",
        "        loss = train_step(model, opt, scaler, micro_input_ids, prec, prof_handler)\n",
        "        total_loss += loss\n",
        "    return total_loss / grad_accum_steps\n",
        "\n",
        "def read_power():\n",
        "    try:\n",
        "        return pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "def _nvml_name_to_str(name_obj):\n",
        "    if isinstance(name_obj, (bytes, bytearray)):\n",
        "        try:\n",
        "            return name_obj.decode()\n",
        "        except Exception:\n",
        "            return str(name_obj)\n",
        "    return str(name_obj)\n",
        "\n",
        "def env_telemetry():\n",
        "    mem_total_gb = pynvml.nvmlDeviceGetMemoryInfo(handle).total / 1024**3\n",
        "    raw_name = pynvml.nvmlDeviceGetName(handle)\n",
        "    name = _nvml_name_to_str(raw_name)\n",
        "    try:\n",
        "        cc = pynvml.nvmlDeviceGetCudaComputeCapability(handle)\n",
        "        cc_str = f\"{cc[0]}.{cc[1]}\"\n",
        "    except Exception:\n",
        "        cc_str = \"unknown\"\n",
        "    return {\n",
        "        \"gpu\": name,\n",
        "        \"mem_total_gb\": mem_total_gb,\n",
        "        \"cc\": cc_str,\n",
        "        \"torch\": torch.__version__\n",
        "    }\n",
        "\n",
        "def cuda_timed(fn):\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "    start.record()\n",
        "    fn()\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return start.elapsed_time(end) / 1000.0\n",
        "\n",
        "def pad_to_multiple(x: torch.Tensor, k: int):\n",
        "    need = x.size(1) % k\n",
        "    if need == 0:\n",
        "        return x\n",
        "    pad = k - need\n",
        "    return torch.nn.functional.pad(x, (0, pad), value=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfdqF3vzrAB6"
      },
      "source": [
        "## Model Builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVZps4ENrAB6"
      },
      "outputs": [],
      "source": [
        "# ===== Model Builder (unchanged from prior fix, kept here for completeness) =====\n",
        "from typing import Tuple\n",
        "import math\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "try:\n",
        "    from transformers import Glm4MoeConfig, Glm4MoeForCausalLM\n",
        "    _HAVE_GLM4_MOE = True\n",
        "except Exception:\n",
        "    _HAVE_GLM4_MOE = False\n",
        "\n",
        "from transformers import Glm4Config, Glm4ForCausalLM\n",
        "\n",
        "def _compute_intermediate_size(hidden_size: int) -> int:\n",
        "    return int(math.ceil((8.0 * hidden_size) / 3.0))\n",
        "\n",
        "def build_glm4_moe_model(\n",
        "    layers: int,\n",
        "    d_model: int,\n",
        "    vocab_size: int = VOCAB,\n",
        "    num_experts: int = EXPERTS,\n",
        "    topk: int = TOPK,\n",
        "    attention_dropout: float = 0.0,\n",
        "    rms_norm_eps: float = 1e-6,\n",
        "    device: str = \"cuda\",\n",
        "    dtype: torch.dtype = torch.float32,\n",
        "):\n",
        "    num_heads = max(8, d_model // 64)\n",
        "    intermediate_size = _compute_intermediate_size(d_model)\n",
        "\n",
        "    if _HAVE_GLM4_MOE:\n",
        "        cfg = Glm4MoeConfig(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=d_model,\n",
        "            num_hidden_layers=layers,\n",
        "            num_attention_heads=num_heads,\n",
        "            hidden_act=\"silu\",\n",
        "            intermediate_size=intermediate_size,\n",
        "            attention_dropout=attention_dropout,\n",
        "            rms_norm_eps=rms_norm_eps,\n",
        "            use_cache=False,\n",
        "            num_experts=num_experts,\n",
        "            topk=topk,\n",
        "            pad_token_id=0,\n",
        "            eos_token_id=2,\n",
        "            bos_token_id=1,\n",
        "        )\n",
        "        model = Glm4MoeForCausalLM(cfg).to(device=device, dtype=dtype)\n",
        "    else:\n",
        "        cfg = Glm4Config(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=d_model,\n",
        "            num_hidden_layers=layers,\n",
        "            num_attention_heads=num_heads,\n",
        "            hidden_act=\"silu\",\n",
        "            intermediate_size=intermediate_size,\n",
        "            attention_dropout=attention_dropout,\n",
        "            rms_norm_eps=rms_norm_eps,\n",
        "            use_cache=False,\n",
        "            pad_token_id=0,\n",
        "            eos_token_id=2,\n",
        "            bos_token_id=1,\n",
        "        )\n",
        "        model = Glm4ForCausalLM(cfg).to(device=device, dtype=dtype)\n",
        "\n",
        "    if (layers * d_model) >= 4096 and hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "        try:\n",
        "            model.gradient_checkpointing_enable()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return model, cfg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wGslOt2rAB6"
      },
      "source": [
        "## Optimized Experiment Runner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmB7fzfjrAB6"
      },
      "outputs": [],
      "source": [
        "# --- Optimized Experiment Runner (RAM-safe polling & compact results) ---\n",
        "\n",
        "def _choose_grad_accum(bs: int, layers: int, d_model: int, seqlen: int, prec: str) -> int:\n",
        "    if d_model >= 512 or seqlen >= 256:\n",
        "        return bs  # micro-batch=1\n",
        "    if bs >= 8 and seqlen >= 128:\n",
        "        return max(1, bs // 4)\n",
        "    return 1\n",
        "\n",
        "def run_one_optimized(layers, d_model, bs, seqlen, prec, compile, static,\n",
        "                     capacity_factor=1.25, grad_accum_steps=1, lr=1e-4,\n",
        "                     prof_enabled=False, model_pool=None, profiler=None):\n",
        "    seed_all()\n",
        "    optimize_gpu_memory()\n",
        "\n",
        "    if model_pool is not None:\n",
        "        model, cfg = model_pool.get_model(layers, d_model, prec=prec)\n",
        "    else:\n",
        "        model, cfg = build_glm4_moe_model(layers, d_model)\n",
        "\n",
        "    if compile:\n",
        "        print(f\"[info] compile requested for ({layers},{d_model},{bs},{seqlen},{prec}) \"\n",
        "              f\"but is disabled on T4 for stability.\")\n",
        "    model_to_run = model\n",
        "\n",
        "    # opt = make_opt(model_to_run, layers, d_model) # Old call with extra args\n",
        "    opt = make_opt(model_to_run) # Corrected call\n",
        "    scaler = torch.amp.GradScaler('cuda') if prec == 'amp' else None\n",
        "\n",
        "    grad_accum_steps = max(grad_accum_steps, _choose_grad_accum(bs, layers, d_model, seqlen, prec))\n",
        "\n",
        "    input_ids_static = None\n",
        "    if static:\n",
        "        static_ids = get_real_batch(bs, seqlen, vocab_size=cfg.vocab_size)\n",
        "        static_ids = pad_to_multiple(static_ids, 8)\n",
        "        input_ids_static = static_ids\n",
        "        validate_batch(input_ids_static, cfg.vocab_size)\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(WARMUP):\n",
        "        if static:\n",
        "            input_ids = input_ids_static\n",
        "        else:\n",
        "            ids = get_real_batch(bs, seqlen, vocab_size=cfg.vocab_size)\n",
        "            ids = pad_to_multiple(ids, 8)\n",
        "            input_ids = ids\n",
        "        _ = train_step_with_grad_accum(model_to_run, opt, scaler, input_ids, prec, grad_accum_steps)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # ---- RAM-safe power/energy tracking: online (no arrays) ----\n",
        "    stop_flag = False\n",
        "    energy_j = 0.0\n",
        "    avg_power = 0.0\n",
        "    n_samples = 0\n",
        "    last_t = time.time()\n",
        "\n",
        "    def poll_power():\n",
        "        nonlocal energy_j, avg_power, n_samples, last_t, stop_flag\n",
        "        while not stop_flag:\n",
        "            p = read_power()\n",
        "            now = time.time()\n",
        "            dt = max(0.0, now - last_t)\n",
        "            last_t = now\n",
        "            energy_j += p * dt\n",
        "            n_samples += 1\n",
        "            # incremental mean to avoid arrays\n",
        "            avg_power += (p - avg_power) / n_samples\n",
        "            time.sleep(POWER_SAMPLE_MS / 1000)\n",
        "\n",
        "    t = threading.Thread(target=poll_power, daemon=True)\n",
        "    t.start()\n",
        "\n",
        "    prof_handler = None\n",
        "    if prof_enabled and ENABLE_PROF:\n",
        "        prof_handler = prof.profile(\n",
        "            activities=[prof.ProfilerActivity.CPU, prof.ProfilerActivity.CUDA],\n",
        "            record_shapes=True, with_stack=True\n",
        "        )\n",
        "        prof_handler.__enter__()\n",
        "\n",
        "    times = []\n",
        "    try:\n",
        "        for r in range(REPEATS):\n",
        "            if static:\n",
        "                input_ids = input_ids_static\n",
        "            else:\n",
        "                ids = get_real_batch(bs, seqlen, vocab_size=cfg.vocab_size)\n",
        "                ids = pad_to_multiple(ids, 8)\n",
        "                input_ids = ids\n",
        "\n",
        "            sec = cuda_timed(lambda: train_step_with_grad_accum(\n",
        "                model_to_run, opt, scaler, input_ids, prec, grad_accum_steps,\n",
        "                (prof_handler if (prof_handler is not None and r == 0) else None)\n",
        "            ))\n",
        "            times.append(sec)\n",
        "\n",
        "            should_stop, _ = statistical_early_stopping(times)\n",
        "            if should_stop and r >= MIN_REPEATS - 1:\n",
        "                break\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "    finally:\n",
        "        if prof_handler is not None:\n",
        "            prof_handler.__exit__(None, None, None)\n",
        "            trace_path = f'trace_l{layers}_d{d_model}_bs{bs}_sl{seqlen}_{prec}.json'\n",
        "            try:\n",
        "                prof_handler.export_chrome_trace(trace_path)\n",
        "            except Exception:\n",
        "                pass\n",
        "        stop_flag = True\n",
        "        t.join(timeout=0.2)\n",
        "\n",
        "    tokens = (input_ids_static.size(0) * input_ids_static.size(1)) if static else (bs * seqlen)\n",
        "    tps = tokens / (sum(times) / len(times)) if times else 0.0\n",
        "    mem_gb = torch.cuda.max_memory_allocated() / 1024**3\n",
        "    env = env_telemetry() if STORE_ENV_PER_EXPERIMENT else None\n",
        "\n",
        "    # Clean up\n",
        "    if scaler is not None:\n",
        "        del scaler\n",
        "    del opt\n",
        "    optimize_gpu_memory()\n",
        "\n",
        "    result = {\n",
        "        'layers': layers,\n",
        "        'd_model': d_model,\n",
        "        'bs': bs,\n",
        "        'seqlen': seqlen,\n",
        "        'prec': prec,\n",
        "        'compile': False,\n",
        "        'static': static,\n",
        "        'grad_accum_steps': grad_accum_steps,\n",
        "        'lr': lr,\n",
        "        'tps': tps,\n",
        "        'mem_gb': mem_gb,\n",
        "        'energy_j': energy_j,\n",
        "        'avg_power_w': avg_power,\n",
        "        'repeats_used': len(times)\n",
        "    }\n",
        "    if STORE_TIMES_PER_RUN:\n",
        "        result['times'] = times\n",
        "    if STORE_ENV_PER_EXPERIMENT:\n",
        "        result['env'] = env\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE3DuXjOrAB7"
      },
      "source": [
        "## Parallel Experiment Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSlphLxPrAB7"
      },
      "outputs": [],
      "source": [
        "# --- Parallel Experiment Execution (stronger OOM guard) ---\n",
        "\n",
        "def _oom_guard(layers, d_model, bs, seqlen, prec, using_adam=True):\n",
        "    # param footprint (with optimizer state)\n",
        "    params = _estimate_params_moe(layers, d_model, EXPERTS, VOCAB)\n",
        "    param_gb = _estimate_param_mem_gb(params, dtype_bytes=4, with_adam=using_adam)\n",
        "    # activation footprint (includes attention seqlen^2 term)\n",
        "    act_gb = _estimate_activation_mem_gb(layers, d_model, bs, seqlen, prec)\n",
        "    # be conservative: params + 2.5x activations + 1GB safety buffer\n",
        "    total_gb = param_gb + 2.5 * act_gb + 1.0\n",
        "    # tighter threshold so we skip borderline configs before they fragment VRAM\n",
        "    return (total_gb > 12.0), total_gb\n",
        "\n",
        "def run_one_cached(*args, cache=None, model_pool=None, profiler=None):\n",
        "    layers, d_model, bs, seqlen, prec, compile, static = args\n",
        "    experiment_key = f\"l{layers}_d{d_model}_bs{bs}_sl{seqlen}_{prec}_c{compile}_s{static}\"\n",
        "\n",
        "    using_adam = (OPTIMIZER_PRIMARY == \"adamw\" and not _needs_low_mem_optimizer(layers, d_model))\n",
        "    should_skip, est_total = _oom_guard(layers, d_model, bs, seqlen, prec, using_adam=using_adam)\n",
        "    if should_skip:\n",
        "        print(f\"Skipping by memory guard (~{est_total:.1f} GB): {args}\")\n",
        "        cache.add_result({\n",
        "            'layers': layers, 'd_model': d_model, 'bs': bs, 'seqlen': seqlen,\n",
        "            'prec': prec, 'compile': compile, 'static': static,\n",
        "            'tps': float('nan'), 'mem_gb': float('nan'),\n",
        "            'energy_j': float('nan'), 'avg_power_w': float('nan'),\n",
        "            'repeats_used': 0,\n",
        "            'error': f\"Skipped by memory guard (~{est_total:.1f} GB)\",\n",
        "            'key': experiment_key\n",
        "        })\n",
        "        return {'key': experiment_key}\n",
        "\n",
        "    if cache.is_completed(experiment_key):\n",
        "        cached = cache.get_result(experiment_key)\n",
        "        return cached if cached else {'key': experiment_key}\n",
        "\n",
        "    prof_enabled = False\n",
        "    if profiler is not None:\n",
        "        prof_enabled = profiler.should_profile(layers, d_model, bs, seqlen)\n",
        "\n",
        "    try:\n",
        "        result = run_one_optimized(*args, prof_enabled=prof_enabled,\n",
        "                                   model_pool=model_pool, profiler=profiler)\n",
        "    except RuntimeError as e:\n",
        "        # catch pool/build errors & CUDA OOM and record as a result line (not crash the run)\n",
        "        cache.add_result({\n",
        "            'layers': layers, 'd_model': d_model, 'bs': bs, 'seqlen': seqlen,\n",
        "            'prec': prec, 'compile': compile, 'static': static,\n",
        "            'tps': float('nan'), 'mem_gb': float('nan'),\n",
        "            'energy_j': float('nan'), 'avg_power_w': float('nan'),\n",
        "            'repeats_used': 0,\n",
        "            'error': str(e),\n",
        "            'key': experiment_key\n",
        "        })\n",
        "        return {'key': experiment_key}\n",
        "\n",
        "    result['key'] = experiment_key\n",
        "    cache.add_result(result)\n",
        "    return result\n",
        "\n",
        "def parallel_experiment_runner(grid, max_workers=None):\n",
        "    max_workers = 1\n",
        "    cache = ExperimentCache()\n",
        "    model_pool = ModelPool()\n",
        "\n",
        "    pending = []\n",
        "    for args in grid:\n",
        "        layers, d_model, bs, seqlen, prec, compile, static = args\n",
        "        key = f\"l{layers}_d{d_model}_bs{bs}_sl{seqlen}_{prec}_c{compile}_s{static}\"\n",
        "        if not cache.is_completed(key):\n",
        "            pending.append(args)\n",
        "\n",
        "    print(f\"Total experiments: {len(grid)}\")\n",
        "    print(f\"Already completed: {len(grid) - len(pending)}\")\n",
        "    print(f\"Pending: {len(pending)}\")\n",
        "\n",
        "    if not pending:\n",
        "        print(\"All experiments already completed!\")\n",
        "        return cache.read_all()\n",
        "\n",
        "    profiler = SmartProfiler()\n",
        "    results_count = 0\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        future_to_args = {\n",
        "            executor.submit(run_one_cached, *args, cache=cache,\n",
        "                            model_pool=model_pool, profiler=profiler): args\n",
        "            for args in pending\n",
        "        }\n",
        "        with tqdm(total=len(future_to_args), desc=\"Running experiments\", leave=True) as pbar:\n",
        "            for future in as_completed(future_to_args):\n",
        "                try:\n",
        "                    _ = future.result()\n",
        "                    results_count += 1\n",
        "                    pbar.set_postfix({'completed': results_count, 'profiles': profiler.profiles_done})\n",
        "                except Exception as e:\n",
        "                    args_failed = future_to_args[future]\n",
        "                    print(f\"Experiment failed: {args_failed}, Error: {e}\")\n",
        "                    layers, d_model, bs, seqlen, prec, compile, static = args_failed\n",
        "                    cache.add_result({\n",
        "                        'layers': layers, 'd_model': d_model, 'bs': bs, 'seqlen': seqlen,\n",
        "                        'prec': prec, 'compile': compile, 'static': static,\n",
        "                        'tps': float('nan'), 'mem_gb': float('nan'),\n",
        "                        'energy_j': float('nan'), 'avg_power_w': float('nan'),\n",
        "                        'repeats_used': 0,\n",
        "                        'error': str(e),\n",
        "                        'key': f\"l{layers}_d{d_model}_bs{bs}_sl{seqlen}_{prec}_c{compile}_s{static}\"\n",
        "                    })\n",
        "                pbar.update(1)\n",
        "                optimize_gpu_memory()\n",
        "\n",
        "    model_pool.clear()\n",
        "    return cache.read_all()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-VVA9_krAB7"
      },
      "source": [
        "## Intelligent Grid Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKMR7w5frAB7",
        "outputId": "90004be4-8fbb-4b4b-f5e2-5410e1c39c7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŽ¯ Intelligent grid generation complete!\n"
          ]
        }
      ],
      "source": [
        "# --- Intelligent Grid Generation (modified/new filtering) ---\n",
        "def create_phased_grids():\n",
        "    # Phase 1: Coarse (kept light on purpose: no d=512)\n",
        "    layers_coarse = INITIAL_LAYERS_LIST\n",
        "    d_model_coarse = INITIAL_D_MODEL_LIST\n",
        "    bs_coarse = INITIAL_BS_LIST\n",
        "    seqlen_coarse = INITIAL_SEQLEN_LIST\n",
        "\n",
        "    grid_coarse = list(itertools.product(\n",
        "        layers_coarse, d_model_coarse, bs_coarse, seqlen_coarse,\n",
        "        PREC_LIST, INITIAL_COMPILE_LIST, INITIAL_STATIC_LIST\n",
        "    ))\n",
        "\n",
        "    # Phase 2: Fine â€” include 512 but only with safer shapes by default\n",
        "    layers_fine = [6, 8, 10, 4, 12]\n",
        "    d_model_fine = [256, 384, 512]\n",
        "    bs_fine = [2, 4, 6, 8]\n",
        "    seqlen_fine = [64, 128, 192, 256]\n",
        "\n",
        "    grid_fine = list(itertools.product(\n",
        "        layers_fine, d_model_fine, bs_fine, seqlen_fine,\n",
        "        PREC_LIST, COMPILE_LIST, STATIC_LIST\n",
        "    ))\n",
        "    return grid_coarse, grid_fine\n",
        "\n",
        "def filter_grid_intelligently(base_grid, results_so_far=None):\n",
        "    if results_so_far is None or len(results_so_far) == 0:\n",
        "        print(f\"ðŸŽ¯ Grid filtered: {len(base_grid)} â†’ {len(base_grid)} (no prior results)\")\n",
        "        return base_grid\n",
        "\n",
        "    def should_include_config(layers, d_model, bs, seqlen, results):\n",
        "        if layers >= 12 and bs >= 8 and seqlen >= 512:\n",
        "            return False\n",
        "        if layers >= 8 and bs >= 16 and seqlen >= 256:\n",
        "            return False\n",
        "        similar = [r for r in results\n",
        "                   if r.get('layers') == layers and r.get('d_model') == d_model\n",
        "                   and not np.isnan(r.get('tps', float('nan')))]\n",
        "        if similar:\n",
        "            avg_tps = np.mean([r['tps'] for r in similar])\n",
        "            if avg_tps < 50:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    filtered = []\n",
        "    for (layers, d_model, bs, seqlen, prec, compile, static) in base_grid:\n",
        "        if should_include_config(layers, d_model, bs, seqlen, results_so_far):\n",
        "            filtered.append((layers, d_model, bs, seqlen, prec, compile, static))\n",
        "    print(f\"ðŸŽ¯ Grid filtered: {len(base_grid)} â†’ {len(filtered)} experiments\")\n",
        "    return filtered\n",
        "\n",
        "def analyze_promising_regions(results):\n",
        "    if not results:\n",
        "        return {}\n",
        "    df = pd.DataFrame(results)\n",
        "    df = df[~df['tps'].isna()]\n",
        "    if len(df) == 0:\n",
        "        return {}\n",
        "    model_performance = df.groupby(['layers', 'd_model'])['tps'].mean().reset_index()\n",
        "    model_performance = model_performance.sort_values('tps', ascending=False)\n",
        "    top_models = model_performance.head(3)\n",
        "    promising_regions = {\n",
        "        'promising_layers': top_models['layers'].tolist(),\n",
        "        'promising_d_models': top_models['d_model'].tolist(),\n",
        "        'best_tps': top_models['tps'].max()\n",
        "    }\n",
        "    print(f\"Promising regions identified: {promising_regions}\")\n",
        "    return promising_regions\n",
        "\n",
        "def filter_grid_by_promising_regions(grid, promising_regions):\n",
        "    if not promising_regions:\n",
        "        return grid\n",
        "    filtered_grid = []\n",
        "    for args in grid:\n",
        "        layers, d_model, bs, seqlen, prec, compile, static = args\n",
        "        if (layers in promising_regions.get('promising_layers', []) or\n",
        "            d_model in promising_regions.get('promising_d_models', [])):\n",
        "            filtered_grid.append(args)\n",
        "    print(f\"Fine grid filtered: {len(grid)} â†’ {len(filtered_grid)} experiments\")\n",
        "    return filtered_grid\n",
        "\n",
        "print(\"ðŸŽ¯ Intelligent grid generation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNollHBvrAB7"
      },
      "source": [
        "## Main Execution Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC_4DxlqrAB7"
      },
      "outputs": [],
      "source": [
        "# --- Main Execution Pipeline (modified to use filter_grid_intelligently on coarse) ---\n",
        "def optimized_evaluation_pipeline(use_phased_approach=True):\n",
        "    print(\"Starting optimized evaluation pipeline\")\n",
        "    print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    cache = ExperimentCache()\n",
        "\n",
        "    if use_phased_approach:\n",
        "        print(\"Using phased approach (coarse â†’ fine)\")\n",
        "\n",
        "        print(\"\\nPhase 1: Coarse grid search\")\n",
        "        grid_coarse, grid_fine = create_phased_grids()\n",
        "\n",
        "        # Read existing results from the cache file\n",
        "        existing_results = cache.read_all()\n",
        "        # IMPORTANT: filter the provided coarse grid, NOT the full grid\n",
        "        grid_coarse_filtered = filter_grid_intelligently(grid_coarse, existing_results)\n",
        "\n",
        "        results_coarse = parallel_experiment_runner(grid_coarse_filtered)\n",
        "\n",
        "        promising_regions = analyze_promising_regions(results_coarse)\n",
        "\n",
        "        print(\"\\nPhase 2: Fine grid search\")\n",
        "        grid_fine_filtered = filter_grid_by_promising_regions(grid_fine, promising_regions)\n",
        "\n",
        "        results_fine = parallel_experiment_runner(grid_fine_filtered)\n",
        "\n",
        "        all_results = results_coarse + results_fine\n",
        "\n",
        "    else:\n",
        "        print(\"Using single-pass approach\")\n",
        "        # If you want a single full pass, just run the full grid directly:\n",
        "        full_grid = list(itertools.product(\n",
        "            LAYERS_LIST, D_MODEL_LIST, BS_LIST, SEQLEN_LIST,\n",
        "            PREC_LIST, COMPILE_LIST, STATIC_LIST\n",
        "        ))\n",
        "        # Read existing results from the cache file\n",
        "        existing_results = cache.read_all()\n",
        "        full_grid_filtered = filter_grid_intelligently(full_grid, existing_results)\n",
        "        all_results = parallel_experiment_runner(full_grid_filtered)\n",
        "\n",
        "    print(f\"Pipeline completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"Total experiments completed: {len(all_results)}\")\n",
        "    return all_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xixwab8rAB8"
      },
      "source": [
        "## Analysis and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXZ4XyQXrAB8"
      },
      "outputs": [],
      "source": [
        "# --- Analysis and Visualization (reads from cache.read_all()) ---\n",
        "\n",
        "def analyze_results(results):\n",
        "    df = pd.DataFrame(results)\n",
        "    if 'tps' not in df or df['tps'].isna().all():\n",
        "        print(\"No valid results to analyze\")\n",
        "        return None, None\n",
        "\n",
        "    df = df[~df['tps'].isna()]\n",
        "\n",
        "    def ci95(x):\n",
        "        return 1.96 * pd.Series(x).std() / math.sqrt(len(x)) if len(x) > 1 else 0\n",
        "\n",
        "    stats = (df.groupby(['layers','d_model','bs','seqlen','prec','compile','static'])\n",
        "           .agg(tps_mean=('tps','mean'),\n",
        "                tps_ci=('tps',ci95),\n",
        "                mem_mean=('mem_gb','mean'),\n",
        "                energy_mean=('energy_j','mean'),\n",
        "                power_mean=('avg_power_w','mean'),\n",
        "                repeats_mean=('repeats_used','mean'))\n",
        "           .reset_index())\n",
        "\n",
        "    summary = stats.pivot_table(\n",
        "        index=['layers','d_model','bs','seqlen','compile','static'],\n",
        "        columns='prec',\n",
        "        values=['tps_mean','tps_ci','mem_mean','energy_mean']\n",
        "    )\n",
        "\n",
        "    if ('tps_mean', 'amp') in summary.columns and ('tps_mean', 'fp32') in summary.columns:\n",
        "        summary['speedup'] = summary[('tps_mean','amp')] / summary[('tps_mean','fp32')]\n",
        "\n",
        "    if ('energy_mean', 'amp') in summary.columns and ('energy_mean', 'fp32') in summary.columns:\n",
        "        summary['energy_ratio'] = summary[('energy_mean','amp')] / summary[('energy_mean','fp32')]\n",
        "\n",
        "    return df, summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYpyNZ6GrAB8"
      },
      "source": [
        "## Save Results and Artefacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tOJ7mgOrAB8"
      },
      "outputs": [],
      "source": [
        "def save_artefacts(df, summary):\n",
        "    os.makedirs(ARTEFACTS_DIR, exist_ok=True)\n",
        "\n",
        "    df.to_csv(f'{ARTEFACTS_DIR}/full_data.csv', index=False)\n",
        "    print(f\"Raw data saved to {ARTEFACTS_DIR}/full_data.csv\")\n",
        "\n",
        "    summary.to_csv(f'{ARTEFACTS_DIR}/summary.csv')\n",
        "    print(f\"Summary saved to {ARTEFACTS_DIR}/summary.csv\")\n",
        "\n",
        "    env_info = env_telemetry()\n",
        "    pr_summary = {\n",
        "        'experiment_info': {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'total_experiments': len(df),\n",
        "            'successful_experiments': len(df[~df['tps'].isna()]),\n",
        "            'configuration': {\n",
        "                'layers_list': LAYERS_LIST,\n",
        "                'd_model_list': D_MODEL_LIST,\n",
        "                'bs_list': BS_LIST,\n",
        "                'seqlen_list': SEQLEN_LIST,\n",
        "                'prec_list': PREC_LIST,\n",
        "                'compile_list': COMPILE_LIST,\n",
        "                'static_list': STATIC_LIST\n",
        "            }\n",
        "        },\n",
        "        'environment': env_info,\n",
        "        'key_findings': {\n",
        "            'avg_speedup': summary['speedup'].mean() if 'speedup' in summary.columns else None,\n",
        "            'avg_energy_ratio': summary['energy_ratio'].mean() if 'energy_ratio' in summary.columns else None,\n",
        "            'best_configuration': summary.loc[summary['speedup'].idxmax()] if 'speedup' in summary.columns else None\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(f'{ARTEFACTS_DIR}/pr_summary.json', 'w') as f:\n",
        "        json.dump(pr_summary, f, indent=2, default=str)\n",
        "\n",
        "    print(f\"PR summary saved to {ARTEFACTS_DIR}/pr_summary.json\")\n",
        "\n",
        "    with open(f'{ARTEFACTS_DIR}/LICENSE', 'w') as f:\n",
        "        f.write('Apache-2.0')\n",
        "\n",
        "    zip_path = f'{ARTEFACTS_DIR}.zip'\n",
        "    shutil.make_archive(ARTEFACTS_DIR, 'zip', ARTEFACTS_DIR)\n",
        "\n",
        "    print(f\"Artefacts bundle created: {zip_path}\")\n",
        "    print(f\"Contains: {len(df)} experiments, {len(summary)} configurations\")\n",
        "\n",
        "    return zip_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaKrfrSGrAB8"
      },
      "source": [
        "## Execute Complete Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88bof7oarAB8"
      },
      "outputs": [],
      "source": [
        "print(\"Starting complete optimized pipeline execution\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    results = optimized_evaluation_pipeline(use_phased_approach=True)\n",
        "\n",
        "    print(\"\\nAnalyzing results...\")\n",
        "    df, summary = analyze_results(results)\n",
        "\n",
        "    print(\"\\nCreating visualizations...\")\n",
        "    create_visualizations(df, summary)\n",
        "\n",
        "    print(\"\\nSaving artefacts...\")\n",
        "    zip_path = save_artefacts(df, summary)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Pipeline execution completed successfully!\")\n",
        "    print(f\"Results available in: {zip_path}\")\n",
        "\n",
        "    if summary is not None and 'speedup' in summary.columns:\n",
        "        print(\"\\nKey Findings:\")\n",
        "        print(f\"   â€¢ Average AMP/FP32 speedup: {summary['speedup'].mean():.2f}x\")\n",
        "        print(f\"   â€¢ Best speedup achieved: {summary['speedup'].max():.2f}x\")\n",
        "        print(f\"   â€¢ Worst speedup observed: {summary['speedup'].min():.2f}x\")\n",
        "\n",
        "        if 'energy_ratio' in summary.columns:\n",
        "            print(f\"   â€¢ Average energy ratio (AMP/FP32): {summary['energy_ratio'].mean():.2f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Pipeline execution failed: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "829d23e1",
        "outputId": "8d5f9be0-97dd-4f2a-8228-e82d84c9e6f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2286993733.py:180: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  by_bins = (pivot.groupby(['size_bin','bs_bin','sl_bin'])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Research bundle ready ===\n",
            "- Artefacts dir: artefacts/\n",
            "- Bundle: /content/artefacts.zip\n",
            "- Key outputs:\n",
            "  â€¢ full_data.csv, matched_pairs.csv, summary_by_config.csv\n",
            "  â€¢ stratified_bins.csv, per_precision_stats.csv\n",
            "  â€¢ regression_log_speedup.csv, key_metrics.json\n",
            "  â€¢ REPORT.md + PNG plots\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Research-Grade Postprocessing\n",
        "# =========================\n",
        "import os, json, math, shutil, datetime, textwrap, statistics, itertools\n",
        "from pathlib import Path\n",
        "from math import comb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Respect prior config if present\n",
        "ARTEFACTS_DIR = os.getenv(\"ARTEFACTS_DIR\", \"artefacts\")\n",
        "CACHE_DIR     = os.getenv(\"CACHE_DIR\", \"experiment_cache\")\n",
        "NDJSON_PATH   = Path(CACHE_DIR) / \"results.ndjson\"\n",
        "os.makedirs(ARTEFACTS_DIR, exist_ok=True)\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _safe_read_ndjson(path: Path) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    if not path.exists():\n",
        "        print(f\"[warn] NDJSON not found at {path}\")\n",
        "        return pd.DataFrame()\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            if not line: continue\n",
        "            try: rows.append(json.loads(line))\n",
        "            except Exception: pass\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def _to_num(df, cols):\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def _ci95(series):\n",
        "    x = pd.Series(series).dropna().values\n",
        "    n = len(x)\n",
        "    if n <= 1: return 0.0\n",
        "    return 1.96 * (np.std(x, ddof=1) / math.sqrt(n))\n",
        "\n",
        "def _bootstrap_ci_mean(vals, B=2000, alpha=0.05, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    vals = np.asarray([v for v in vals if np.isfinite(v)])\n",
        "    if len(vals) == 0: return (np.nan, np.nan)\n",
        "    boots = []\n",
        "    n = len(vals)\n",
        "    for _ in range(B):\n",
        "        sample = vals[rng.integers(0, n, size=n)]\n",
        "        boots.append(np.mean(sample))\n",
        "    boots = np.sort(boots)\n",
        "    lo = np.percentile(boots, 100*alpha/2)\n",
        "    hi = np.percentile(boots, 100*(1-alpha/2))\n",
        "    return (float(lo), float(hi))\n",
        "\n",
        "def _binomial_p_two_sided(k, n, p=0.5):\n",
        "    # exact two-sided binomial (small n safe)\n",
        "    # sum of probabilities of outcomes as or more extreme than observed\n",
        "    def pmf(k, n, p):\n",
        "        return comb(n, k) * (p**k) * ((1-p)**(n-k))\n",
        "    # central value\n",
        "    obs = pmf(k, n, p)\n",
        "    tot = 0.0\n",
        "    for i in range(0, n+1):\n",
        "        if pmf(i, n, p) <= obs + 1e-18:\n",
        "            tot += pmf(i, n, p)\n",
        "    return min(1.0, 2.0*min(sum(pmf(i,n,p) for i in range(0,k+1)),\n",
        "                             sum(pmf(i,n,p) for i in range(k,n+1))))\n",
        "\n",
        "def _winsorize(series, p=0.01):\n",
        "    x = pd.Series(series).astype(float)\n",
        "    lo, hi = x.quantile(p), x.quantile(1-p)\n",
        "    return x.clip(lo, hi)\n",
        "\n",
        "def _fmt(x, n=3):\n",
        "    try: return f\"{float(x):.{n}g}\"\n",
        "    except: return str(x)\n",
        "\n",
        "def _maybe_env():\n",
        "    try:\n",
        "        return env_telemetry()  # defined earlier in your notebook\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "# ---------- 1) Load + clean ----------\n",
        "raw = _safe_read_ndjson(NDJSON_PATH)\n",
        "if raw.empty:\n",
        "    raise RuntimeError(\"No results.ndjson found or it was empty. Run experiments first.\")\n",
        "\n",
        "num_cols = [\"layers\",\"d_model\",\"bs\",\"seqlen\",\"tps\",\"mem_gb\",\"energy_j\",\"avg_power_w\",\"repeats_used\"]\n",
        "df = _to_num(raw.copy(), num_cols)\n",
        "for c in ['compile','static','prec']:\n",
        "    if c not in df.columns:\n",
        "        df[c] = False if c != 'prec' else np.nan\n",
        "\n",
        "# Minimal valid rows\n",
        "df = df[df[\"tps\"].notna() & df[\"layers\"].notna() & df[\"d_model\"].notna() &\n",
        "        df[\"bs\"].notna() & df[\"seqlen\"].notna() & df[\"prec\"].notna()].copy()\n",
        "\n",
        "# ---------- 2) Matched AMPâ†”FP32 pairs ----------\n",
        "pair_keys = ['layers','d_model','bs','seqlen','compile','static']\n",
        "pivot = (df\n",
        "         .groupby(pair_keys + ['prec'])[['tps','energy_j']] # Corrected: ['tps','energy_j'] is now a list\n",
        "         .mean().unstack('prec'))\n",
        "pivot.columns = [f\"{m}__{p}\" for m,p in pivot.columns]\n",
        "pivot = pivot.reset_index()\n",
        "\n",
        "# keep only pairs where both exist\n",
        "need = ['tps__amp','tps__fp32']\n",
        "pivot = pivot[[*pair_keys, *[c for c in pivot.columns if c in need or c.startswith(\"energy_j__\")]]].dropna(subset=need)\n",
        "\n",
        "# Metrics: ratios/percent diffs; use log for stability\n",
        "pivot['speedup'] = pivot['tps__amp'] / pivot['tps__fp32']\n",
        "pivot['log_speedup'] = np.log(pivot['speedup'])\n",
        "if 'energy_j__amp' in pivot.columns and 'energy_j__fp32' in pivot.columns:\n",
        "    pivot['energy_ratio'] = pivot['energy_j__amp'] / pivot['energy_j__fp32']\n",
        "else:\n",
        "    pivot['energy_ratio'] = np.nan\n",
        "\n",
        "# Winsorize extreme ratios for analysis plots\n",
        "pivot['speedup_w'] = _winsorize(pivot['speedup'])\n",
        "pivot['log_speedup_w'] = np.log(pivot['speedup_w'])\n",
        "\n",
        "# ---------- 3) Descriptive stats ----------\n",
        "overall_avg_speed = float(np.mean(pivot['speedup']))\n",
        "overall_med_speed = float(np.median(pivot['speedup']))\n",
        "mean_log = float(np.mean(pivot['log_speedup']))\n",
        "ci_lo, ci_hi = _bootstrap_ci_mean(pivot['log_speedup'], B=2000, alpha=0.05)\n",
        "\n",
        "# Convert log-CI to multiplicative CI\n",
        "mul_lo, mul_hi = math.exp(ci_lo), math.exp(ci_hi)\n",
        "\n",
        "# Sign test: count configs where AMP > FP32\n",
        "wins = int(np.sum(pivot['speedup'] > 1.0))\n",
        "loss = int(np.sum(pivot['speedup'] < 1.0))\n",
        "ties = int(np.sum(np.isclose(pivot['speedup'], 1.0, atol=1e-3)))\n",
        "n_eff = wins + loss\n",
        "p_two = _binomial_p_two_sided(wins, n_eff, p=0.5) if n_eff > 0 else np.nan\n",
        "\n",
        "# ---------- 4) Simple regression on log-speedup ----------\n",
        "# features: intercept, layers, d_model, log(bs), log(seqlen), static (0/1)\n",
        "X_cols = []\n",
        "X = []\n",
        "for _, r in pivot.iterrows():\n",
        "    X_cols = ['intercept','layers','d_model','log_bs','log_seqlen','static']\n",
        "    X.append([\n",
        "        1.0,\n",
        "        float(r['layers']),\n",
        "        float(r['d_model']),\n",
        "        math.log(max(1.0, float(r['bs']))),\n",
        "        math.log(max(1.0, float(r['seqlen']))),\n",
        "        1.0 if bool(r['static']) else 0.0\n",
        "    ])\n",
        "X = np.asarray(X)\n",
        "y = pivot['log_speedup'].values\n",
        "# OLS via lstsq (no regularization)\n",
        "coefs, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
        "reg = pd.DataFrame({'feature': X_cols, 'coef': coefs})\n",
        "# crude standard errors via classical formula\n",
        "y_hat = X @ coefs\n",
        "resid = y - y_hat\n",
        "dof = max(1, len(y) - len(coefs))\n",
        "sigma2 = float(np.sum(resid**2) / dof)\n",
        "cov = sigma2 * np.linalg.inv(X.T @ X)\n",
        "se = np.sqrt(np.diag(cov))\n",
        "reg['stderr'] = se\n",
        "reg['t'] = reg['coef'] / reg['stderr']\n",
        "reg['exp_coef'] = np.exp(reg['coef'])  # multiplicative effect on speedup\n",
        "reg_path = Path(ARTEFACTS_DIR) / \"regression_log_speedup.csv\"\n",
        "reg.to_csv(reg_path, index=False)\n",
        "\n",
        "# ---------- 5) Stratified tables ----------\n",
        "# bins\n",
        "size_proxy = pivot['layers'] * pivot['d_model']\n",
        "pivot['size_bin'] = pd.qcut(size_proxy, q=min(5, max(2, size_proxy.nunique())), duplicates='drop')\n",
        "pivot['bs_bin'] = pd.cut(pivot['bs'], bins=sorted(set([1,2,4,6,8,12,16,32])), right=True, include_lowest=True)\n",
        "pivot['sl_bin'] = pd.cut(pivot['seqlen'], bins=sorted(set([64,96,128,160,192,224,256,320,384,512])), right=True, include_lowest=True)\n",
        "\n",
        "by_bins = (pivot.groupby(['size_bin','bs_bin','sl_bin'])\n",
        "                 .agg(n=('speedup','size'),\n",
        "                      speedup_mean=('speedup','mean'),\n",
        "                      speedup_med=('speedup','median'),\n",
        "                      log_speedup_mean=('log_speedup','mean'))\n",
        "                 .reset_index())\n",
        "by_bins_path = Path(ARTEFACTS_DIR) / \"stratified_bins.csv\"\n",
        "by_bins.to_csv(by_bins_path, index=False)\n",
        "\n",
        "# ---------- 6) Per-precision summaries (sanity) ----------\n",
        "per_prec = (df.groupby(['layers','d_model','bs','seqlen','prec'])['tps']\n",
        "              .agg(['count','mean','median']).reset_index())\n",
        "per_prec_path = Path(ARTEFACTS_DIR) / \"per_precision_stats.csv\"\n",
        "per_prec.to_csv(per_prec_path, index=False)\n",
        "\n",
        "# ---------- 7) Plots ----------\n",
        "ts = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "# Heatmap: speedup by (model) x (workload)\n",
        "try:\n",
        "    heat = pivot.copy()\n",
        "    heat['model'] = heat['layers'].astype(int).astype(str) + \"LÃ—\" + heat['d_model'].astype(int).astype(str)\n",
        "    heat['workload'] = \"bs\" + heat['bs'].astype(int).astype(str) + \"_sl\" + heat['seqlen'].astype(int).astype(str)\n",
        "    mat = heat.pivot_table(index='model', columns='workload', values='speedup', aggfunc='mean')\n",
        "    plt.figure(figsize=(12, max(4, 0.5*len(mat))))\n",
        "    im = plt.imshow(mat, aspect='auto')\n",
        "    plt.colorbar(im, label='AMP / FP32 speedup (â†‘ better)')\n",
        "    plt.title('AMP vs FP32 Speedup by Model & Workload')\n",
        "    plt.yticks(range(len(mat.index)), mat.index)\n",
        "    plt.xticks(range(len(mat.columns)), mat.columns, rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{ARTEFACTS_DIR}/speedup_heatmap_{ts}.png\", dpi=150, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "except Exception as e:\n",
        "    print(f\"[warn] heatmap failed: {e}\")\n",
        "\n",
        "# Scatter: speedup vs seqlen / bs\n",
        "try:\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.scatter(pivot['seqlen'], pivot['speedup_w'], alpha=0.6, s=32)\n",
        "    plt.axhline(1.0, color='r', linestyle='--')\n",
        "    plt.xlabel(\"Sequence length\")\n",
        "    plt.ylabel(\"AMP/FP32 speedup (winsorized)\")\n",
        "    plt.title(\"Speedup vs Sequence Length\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{ARTEFACTS_DIR}/speedup_vs_seqlen_{ts}.png\", dpi=150, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.scatter(pivot['bs'], pivot['speedup_w'], alpha=0.6, s=32)\n",
        "    plt.axhline(1.0, color='r', linestyle='--')\n",
        "    plt.xlabel(\"Batch size\")\n",
        "    plt.ylabel(\"AMP/FP32 speedup (winsorized)\")\n",
        "    plt.title(\"Speedup vs Batch Size\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{ARTEFACTS_DIR}/speedup_vs_bs_{ts}.png\", dpi=150, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "except Exception as e:\n",
        "    print(f\"[warn] scatter plots failed: {e}\")\n",
        "\n",
        "# Top tables like before\n",
        "top_tps = (df.sort_values('tps', ascending=False)\n",
        "             .head(20)\n",
        "             [['layers','d_model','bs','seqlen','prec','tps','mem_gb','avg_power_w']])\n",
        "top_speedup = (pivot.sort_values('speedup', ascending=False)\n",
        "                    .head(20)\n",
        "                    [['layers','d_model','bs','seqlen','compile','static','speedup']])\n",
        "top_tps.to_csv(Path(ARTEFACTS_DIR)/\"top20_tps.csv\", index=False)\n",
        "top_speedup.to_csv(Path(ARTEFACTS_DIR)/\"top20_speedup.csv\", index=False)\n",
        "\n",
        "# ---------- 8) Save tidy datasets ----------\n",
        "# (a) raw row-level data\n",
        "df_csv = Path(ARTEFACTS_DIR)/\"full_data.csv\"\n",
        "df.to_csv(df_csv, index=False)\n",
        "try: df.to_parquet(Path(ARTEFACTS_DIR)/\"full_data.parquet\", index=False)\n",
        "except Exception: pass\n",
        "\n",
        "# (b) matched pairs\n",
        "pairs_csv = Path(ARTEFACTS_DIR)/\"matched_pairs.csv\"\n",
        "pivot.to_csv(pairs_csv, index=False)\n",
        "\n",
        "# (c) compact summary\n",
        "stats = (df.groupby(['layers','d_model','bs','seqlen','prec','compile','static'])\n",
        "           .agg(tps_mean=('tps','mean'),\n",
        "                tps_ci=('tps', _ci95),\n",
        "                mem_mean=('mem_gb','mean'),\n",
        "                energy_mean=('energy_j','mean'),\n",
        "                power_mean=('avg_power_w','mean'),\n",
        "                repeats_mean=('repeats_used','mean'))\n",
        "           .reset_index())\n",
        "summary = stats.pivot_table(\n",
        "    index=['layers','d_model','bs','seqlen','compile','static'],\n",
        "    columns='prec',\n",
        "    values=['tps_mean','tps_ci','mem_mean','energy_mean'],\n",
        "    aggfunc='first'\n",
        ")\n",
        "if ('tps_mean','amp') in summary.columns and ('tps_mean','fp32') in summary.columns:\n",
        "    summary['speedup'] = summary[('tps_mean','amp')] / summary[('tps_mean','fp32')]\n",
        "if ('energy_mean','amp') in summary.columns and ('energy_mean','fp32') in summary.columns:\n",
        "    summary['energy_ratio'] = summary[('energy_mean','amp')] / summary[('energy_mean','fp32')]\n",
        "summary_path = Path(ARTEFACTS_DIR)/\"summary_by_config.csv\"\n",
        "# flatten for CSV\n",
        "flat_sum = summary.copy()\n",
        "flat_sum.columns = ['__'.join(c).strip('_') if isinstance(c, tuple) else c for c in flat_sum.columns]\n",
        "flat_sum.reset_index().to_csv(summary_path, index=False)\n",
        "\n",
        "# ---------- 9) Key metrics JSON ----------\n",
        "env_info = _maybe_env()\n",
        "key = {\n",
        "    \"generated_at\": datetime.datetime.now().isoformat(timespec='seconds'),\n",
        "    \"environment\": env_info,\n",
        "    \"counts\": {\n",
        "        \"rows\": int(len(df)),\n",
        "        \"pairs\": int(len(pivot)),\n",
        "        \"unique_settings\": int(df[['layers','d_model','bs','seqlen','prec']].drop_duplicates().shape[0])\n",
        "    },\n",
        "    \"speedup\": {\n",
        "        \"avg_ratio\": overall_avg_speed,\n",
        "        \"median_ratio\": overall_med_speed,\n",
        "        \"log_mean\": mean_log,\n",
        "        \"log_mean_ci_95\": [ci_lo, ci_hi],\n",
        "        \"mult_mean_ci_95\": [mul_lo, mul_hi]\n",
        "    },\n",
        "    \"sign_test\": {\n",
        "        \"wins\": wins, \"losses\": loss, \"ties\": ties, \"two_sided_p\": float(p_two)\n",
        "    },\n",
        "    \"regression_log_speedup\": reg.to_dict(orient=\"records\")\n",
        "}\n",
        "with open(Path(ARTEFACTS_DIR)/\"key_metrics.json\",\"w\") as f:\n",
        "    json.dump(key, f, indent=2)\n",
        "\n",
        "# ---------- 10) Research-ready REPORT.md ----------\n",
        "report = Path(ARTEFACTS_DIR)/\"REPORT.md\"\n",
        "with open(report, \"w\") as f:\n",
        "    f.write(\"# T4 AMP vs FP32 â€” Research Report\\n\\n\")\n",
        "    f.write(f\"_Generated: {datetime.datetime.now().isoformat(timespec='seconds')}_\\n\\n\")\n",
        "\n",
        "    if env_info:\n",
        "        f.write(\"## Environment\\n\")\n",
        "        f.write(f\"- GPU: {env_info.get('gpu','?')}\\n\")\n",
        "        f.write(f\"- Memory: {_fmt(env_info.get('mem_total_gb','?'))} GB\\n\")\n",
        "        f.write(f\"- Compute Capability: {env_info.get('cc','?')}\\n\")\n",
        "        f.write(f\"- PyTorch: {env_info.get('torch','?')}\\n\\n\")\n",
        "\n",
        "    f.write(\"## Dataset Stats\\n\")\n",
        "    f.write(f\"- Total rows: {len(df)}\\n\")\n",
        "    f.write(f\"- Matched AMPâ†”FP32 pairs: {len(pivot)}\\n\")\n",
        "    uniq = df[['layers','d_model','bs','seqlen','prec']].drop_duplicates().shape[0]\n",
        "    f.write(f\"- Unique (layers,d_model,bs,seqlen,prec) combos: {uniq}\\n\\n\")\n",
        "\n",
        "    f.write(\"## Executive Summary\\n\")\n",
        "    f.write(f\"- Mean AMP/FP32 speedup across **matched** configs: **{_fmt(overall_avg_speed,3)}Ã—** \"\n",
        "            f\"(median **{_fmt(overall_med_speed,3)}Ã—**).\\n\")\n",
        "    f.write(f\"- Mean log-speedup 95% bootstrap CI: **[{_fmt(mul_lo,3)}, {_fmt(mul_hi,3)}]Ã—**.\\n\")\n",
        "    if not math.isnan(p_two):\n",
        "        f.write(f\"- Sign test vs parity (AMP=FP32): wins={wins}, losses={loss}, ties={ties}, \"\n",
        "                f\"two-sided pâ‰ˆ**{_fmt(p_two,3)}**.\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    f.write(\"## When AMP Helps (Patterns)\\n\")\n",
        "    # quick rules of thumb from regression (exp_coef > 1 means positive on speedup)\n",
        "    f.write(\"Interpreting coefficients on **log-speedup** (multiplicative effects):\\n\\n\")\n",
        "    f.write(reg.rename(columns={'exp_coef':'mult_effect'})\n",
        "            [['feature','coef','stderr','t','mult_effect']]\n",
        "            .to_markdown(index=False))\n",
        "    f.write(\"\\n\\n\")\n",
        "\n",
        "    f.write(\"## Top Throughput (absolute TPS)\\n\")\n",
        "    f.write(top_tps.to_markdown(index=False))\n",
        "    f.write(\"\\n\\n\")\n",
        "\n",
        "    f.write(\"## Top AMP Speedups (AMP/FP32)\\n\")\n",
        "    f.write(top_speedup.rename(columns={'speedup':'AMP/FP32'}).to_markdown(index=False))\n",
        "    f.write(\"\\n\\n\")\n",
        "\n",
        "    f.write(\"## Stratified Effects (mean speedup)\\n\")\n",
        "    head = by_bins.head(20).copy()\n",
        "    f.write(head.to_markdown(index=False))\n",
        "    f.write(\"\\n\\n\")\n",
        "\n",
        "    f.write(\"## Plots\\n\")\n",
        "    f.write(\"- `speedup_heatmap_*.png` (AMP/FP32 by model Ã— workload)\\n\")\n",
        "    f.write(\"- `speedup_vs_seqlen_*.png`\\n\")\n",
        "    f.write(\"- `speedup_vs_bs_*.png`\\n\\n\")\n",
        "\n",
        "    f.write(\"## Methods\\n\")\n",
        "    f.write(textwrap.dedent(\"\"\"\n",
        "    - We form matched pairs on (layers, d_model, bs, seqlen, compile, static).\n",
        "    - Throughput (TPS) is averaged per (precision Ã— setting) before pairing.\n",
        "    - Speedup = TPS_amp / TPS_fp32. To stabilize variance, analyses use log-speedup.\n",
        "    - Confidence intervals: bootstrap (B=2000) on mean log-speedup, then exponentiated.\n",
        "    - Hypothesis test: exact two-sided binomial sign test on wins vs losses (ties dropped).\n",
        "    - Regression: OLS of log-speedup on layers, d_model, log(bs), log(seqlen), static.\n",
        "    \"\"\").strip() + \"\\n\\n\")\n",
        "\n",
        "    f.write(\"## Limitations\\n\")\n",
        "    f.write(\"- Synthetic tokens; real datasets may shift memory/bandwidth balance.\\n\")\n",
        "    f.write(\"- Single GPU model (T4); other architectures may differ.\\n\")\n",
        "    f.write(\"- No kernel-level attribution; profiler traces were disabled by default.\\n\\n\")\n",
        "\n",
        "    f.write(\"## Recommendations (T4-specific)\\n\")\n",
        "    f.write(\"- Prefer AMP when small models and short sequences dominate (see top speedups).\\n\")\n",
        "    f.write(\"- For high throughput targets, tune batch size first; compare AMP vs FP32 on the tuned point.\\n\")\n",
        "    f.write(\"- If AMP underperforms, try enabling static padding and/or reducing sequence length.\\n\")\n",
        "\n",
        "# ---------- 11) Bundle ----------\n",
        "bundle = shutil.make_archive(ARTEFACTS_DIR, \"zip\", ARTEFACTS_DIR)\n",
        "\n",
        "print(\"\\n=== Research bundle ready ===\")\n",
        "print(f\"- Artefacts dir: {ARTEFACTS_DIR}/\")\n",
        "print(f\"- Bundle: {bundle}\")\n",
        "print(\"- Key outputs:\")\n",
        "print(\"  â€¢ full_data.csv, matched_pairs.csv, summary_by_config.csv\")\n",
        "print(\"  â€¢ stratified_bins.csv, per_precision_stats.csv\")\n",
        "print(\"  â€¢ regression_log_speedup.csv, key_metrics.json\")\n",
        "print(\"  â€¢ REPORT.md + PNG plots\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
