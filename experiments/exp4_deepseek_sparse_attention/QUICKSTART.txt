â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘             EXPERIMENT 4: DeepSeek Sparse vs Classic Attention            â•‘
â•‘                     Sequence Length Comparison                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸš€ QUICKSTART
-------------
  cd experiments/exp4_deepseek_sparse_attention
  python run_experiment.py


ğŸ“Š VIEW RESULTS
---------------
  Main plot:     results/sequence_length_comparison.png
  Summary:       results/summary.json
  Details:       results/RESULTS_SUMMARY.md


ğŸ¯ WHAT'S TESTED
----------------
  â€¢ DeepSeek Sparse Attention (from paper) vs Classic Dense Attention
  â€¢ Sequence lengths: 64, 128, 256 tokens
  â€¢ Same architecture, fair comparison
  â€¢ 1000 training steps per model


âœ… KEY FINDING
--------------
  Sparse Attention WINS across all sequence lengths:
    - 48-71% better validation loss
    - 5-11x better accuracy  
    - Similar training speed (~0.06-0.07s per step)


ğŸ“ FILES (MINIMAL & CLEAN)
---------------------------
  run_experiment.py         Main experiment script
  exp4_models.py            Model definitions (sparse + classic)
  sparse_attention.py       DeepSeek sparse attention implementation
  config.py                 Configuration helpers
  README.md                 Full documentation
  EXPERIMENT_OVERVIEW.md    Quick reference


ğŸ”§ CUSTOMIZE
------------
  Edit run_experiment.py:
    - SEQUENCE_LENGTHS = [64, 128, 256]   # Which lengths to test
    - BASE_CONFIG['steps'] = 1000          # Training steps
    - BASE_CONFIG['d_model'] = 256         # Model size


ğŸ’¡ HOW IT WORKS
---------------
  Classic Attention:
    â€¢ Standard multi-head attention
    â€¢ O(LÂ²) complexity (quadratic scaling)
    â€¢ Attends to all tokens
  
  DeepSeek Sparse Attention:
    â€¢ Lightning indexer selects relevant tokens
    â€¢ O(Lk) complexity where k < L
    â€¢ Only attends to top-k tokens (50% in this experiment)
    â€¢ Much better performance with similar speed!


ğŸ“ˆ RESULTS SUMMARY
------------------
  Best Results (Seq Length = 256):
    Classic:  Loss=7.05  Acc=7.9%   Time=0.063s/step
    Sparse:   Loss=2.49  Acc=61.0%  Time=0.067s/step âœ… WINNER
  
  Improvement: 71% better loss, 8x better accuracy!


ğŸ“š BASED ON
-----------
  DeepSeek-V3 Paper (DeepSeek_V3_2.pdf in repo root)
  Lightning Indexer + Top-k Selection + Sparse Attention


ğŸ” FOR MORE INFO
----------------
  â€¢ README.md - Full documentation
  â€¢ EXPERIMENT_OVERVIEW.md - Quick reference
  â€¢ results/RESULTS_SUMMARY.md - Detailed results analysis
  â€¢ DeepSeek_V3_2.pdf - Original paper

