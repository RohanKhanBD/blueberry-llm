# Experiment 7: Gated DeltaNet Training Requirements
# Install with: pip install -r requirements.txt

# FLA - Flash Linear Attention (optimized DeltaNet kernels)
git+https://github.com/fla-org/flash-linear-attention

# Flash Attention - Required for full transformer baseline
# Note: Install with: pip install flash-attn --no-build-isolation
flash-attn
