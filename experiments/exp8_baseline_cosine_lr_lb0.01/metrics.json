{
  "experiment_config": {
    "name": "exp8_baseline_cosine_lr_lb0.01",
    "description": "Baseline 1000-step run with cosine LR schedule and load balancing weight 0.01 - Shows validation loss inflection at step ~850",
    "max_steps": 1000,
    "lr_schedule_type": "cosine",
    "use_early_stopping": false,
    "load_balancing_weight": 0.01,
    "dropout": 0.1,
    "batch_size": 24,
    "num_experts": 8,
    "expert_top_k": 2
  },
  "final_metrics": {
    "val_loss": 5.15637372642435,
    "val_accuracy": 0.2519060945927937,
    "val_perplexity": 173.53403137812282
  },
  "best_metrics": {
    "val_loss": 5.1357,
    "step": 950,
    "val_accuracy": 0.252
  },
  "total_time_minutes": 3.83,
  "stopped_early": false,
  "actual_steps": 1000,
  "history": {
    "steps": [
      10,
      20,
      30,
      40,
      50,
      60,
      70,
      80,
      90,
      100,
      110,
      120,
      130,
      140,
      150,
      160,
      170,
      180,
      190,
      200,
      210,
      220,
      230,
      240,
      250,
      260,
      270,
      280,
      290,
      300,
      310,
      320,
      330,
      340,
      350,
      360,
      370,
      380,
      390,
      400,
      410,
      420,
      430,
      440,
      450,
      460,
      470,
      480,
      490,
      500,
      510,
      520,
      530,
      540,
      550,
      560,
      570,
      580,
      590,
      600,
      610,
      620,
      630,
      640,
      650,
      660,
      670,
      680,
      690,
      700,
      710,
      720,
      730,
      740,
      750,
      760,
      770,
      780,
      790,
      800,
      810,
      820,
      830,
      840,
      850,
      860,
      870,
      880,
      890,
      900,
      910,
      920,
      930,
      940,
      950,
      960,
      970,
      980,
      990,
      1000
    ],
    "val_losses": [
      10.7883,
      10.7477,
      10.6945,
      10.5719,
      10.4574,
      10.2212,
      10.0088,
      9.6009,
      9.2832,
      8.7873,
      8.47,
      8.0336,
      7.7844,
      7.5064,
      7.4044,
      7.3558,
      7.3275,
      7.1881,
      7.0853,
      6.9538,
      6.8585,
      6.7464,
      6.6771,
      6.5806,
      6.5227,
      6.4483,
      6.404,
      6.3411,
      6.3042,
      6.2522,
      6.2222,
      6.1798,
      6.1584,
      6.1299,
      6.1111,
      6.0845,
      6.0643,
      6.0322,
      6.017,
      5.9836,
      5.9661,
      5.9352,
      5.9238,
      5.9076,
      5.9011,
      5.8707,
      5.8561,
      5.8376,
      5.8158,
      5.7852,
      5.7693,
      5.7422,
      5.7203,
      5.7088,
      5.6975,
      5.6805,
      5.6682,
      5.6458,
      5.6312,
      5.605,
      5.587,
      5.5565,
      5.5374,
      5.5161,
      5.5083,
      5.4978,
      5.4843,
      5.4652,
      5.4488,
      5.4258,
      5.4084,
      5.3841,
      5.3684,
      5.3432,
      5.3432,
      5.337,
      5.3288,
      5.3168,
      5.3069,
      5.2894,
      5.2783,
      5.2563,
      5.2451,
      5.2215,
      5.2094,
      5.2161,
      5.2188,
      5.2154,
      5.2118,
      5.2003,
      5.1904,
      5.1735,
      5.1611,
      5.1461,
      5.1357,
      5.1375,
      5.1488,
      5.1567,
      5.1575,
      5.1564
    ],
    "val_accuracies": [
      0.0153,
      0.0153,
      0.0153,
      0.0153,
      0.0153,
      0.0153,
      0.0153,
      0.0153,
      0.0188,
      0.0663,
      0.0883,
      0.089,
      0.0877,
      0.0915,
      0.098,
      0.1214,
      0.1283,
      0.1356,
      0.1429,
      0.1492,
      0.1527,
      0.1587,
      0.1615,
      0.1638,
      0.1647,
      0.167,
      0.1679,
      0.1695,
      0.1696,
      0.172,
      0.1725,
      0.1741,
      0.1747,
      0.1751,
      0.1756,
      0.1769,
      0.1775,
      0.1782,
      0.1779,
      0.1795,
      0.1801,
      0.1824,
      0.1825,
      0.1849,
      0.1856,
      0.1876,
      0.1887,
      0.1908,
      0.192,
      0.1949,
      0.1968,
      0.1983,
      0.2003,
      0.2012,
      0.202,
      0.2042,
      0.2051,
      0.2065,
      0.2075,
      0.2094,
      0.2121,
      0.2142,
      0.215,
      0.2166,
      0.2182,
      0.2189,
      0.2206,
      0.2218,
      0.2234,
      0.2252,
      0.2271,
      0.2295,
      0.2306,
      0.2325,
      0.2321,
      0.2331,
      0.2338,
      0.2351,
      0.236,
      0.2382,
      0.2397,
      0.2405,
      0.2412,
      0.2436,
      0.2455,
      0.2444,
      0.2443,
      0.2446,
      0.2451,
      0.2461,
      0.2465,
      0.248,
      0.2496,
      0.2516,
      0.252,
      0.2518,
      0.2511,
      0.2517,
      0.2524,
      0.2519
    ],
    "val_perplexities": [
      48450.45,
      46522.28,
      44113.63,
      39024.02,
      34801.74,
      27479.31,
      22222.22,
      14778.68,
      10756.12,
      6550.5,
      4769.43,
      3082.84,
      2402.9,
      1819.71,
      1643.26,
      1565.2,
      1521.51,
      1323.58,
      1194.24,
      1047.12,
      951.97,
      850.99,
      794.03,
      721.0,
      680.42,
      631.65,
      604.29,
      567.41,
      546.85,
      519.14,
      503.81,
      482.89,
      472.69,
      459.37,
      450.85,
      438.99,
      430.2,
      416.64,
      410.33,
      396.87,
      389.99,
      378.11,
      373.82,
      367.83,
      365.43,
      354.51,
      349.35,
      342.95,
      335.57,
      325.44,
      320.31,
      311.74,
      304.99,
      301.5,
      298.12,
      293.09,
      289.51,
      283.11,
      278.99,
      271.79,
      266.93,
      258.91,
      254.01,
      248.66,
      246.73,
      244.16,
      240.89,
      236.32,
      232.48,
      227.19,
      223.28,
      217.9,
      214.52,
      209.19,
      209.17,
      207.9,
      206.2,
      203.74,
      201.72,
      198.22,
      196.04,
      191.77,
      189.64,
      185.21,
      182.99,
      184.21,
      184.72,
      184.09,
      183.43,
      181.33,
      179.54,
      176.53,
      174.36,
      171.76,
      169.98,
      170.3,
      172.23,
      173.59,
      173.73,
      173.53
    ],
    "elapsed_times": [
      0.0,
      0.0383,
      0.0766,
      0.1149,
      0.1532,
      0.1915,
      0.2298,
      0.2681,
      0.3064,
      0.3447,
      0.383,
      0.4213,
      0.4596,
      0.4979,
      0.5362,
      0.5745,
      0.6128,
      0.6511,
      0.6894,
      0.7277,
      0.766,
      0.8043,
      0.8426,
      0.8809,
      0.9192,
      0.9575,
      0.9958,
      1.0341,
      1.0724,
      1.1107,
      1.149,
      1.1873,
      1.2256,
      1.2639,
      1.3022,
      1.3405,
      1.3788,
      1.4171,
      1.4554,
      1.4937,
      1.532,
      1.5703,
      1.6086,
      1.6469,
      1.6852,
      1.7235,
      1.7618,
      1.8001,
      1.8384,
      1.8767,
      1.915,
      1.9533,
      1.9916,
      2.0299,
      2.0682,
      2.1065,
      2.1448,
      2.1831,
      2.2214,
      2.2597,
      2.298,
      2.3363,
      2.3746,
      2.4129,
      2.4512,
      2.4895,
      2.5278,
      2.5661,
      2.6044,
      2.6427,
      2.681,
      2.7193,
      2.7576,
      2.7959,
      2.8342,
      2.8725,
      2.9108,
      2.9491,
      2.9874,
      3.0257,
      3.064,
      3.1023,
      3.1406,
      3.1789,
      3.2172,
      3.2555,
      3.2938,
      3.3321,
      3.3704,
      3.4087,
      3.447,
      3.4853,
      3.5236,
      3.5619,
      3.6002,
      3.6385,
      3.6768,
      3.7151,
      3.7534,
      3.7917
    ],
    "learning_rates": [
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01
    ]
  },
  "notes": [
    "This is the baseline run showing the validation loss inflection issue",
    "Best val loss: 5.1357 at step 950",
    "Final val loss: 5.1564 at step 1000",
    "Loss increased by ~0.02 in final 50 steps, suggesting overfitting or LR schedule issues"
  ]
}